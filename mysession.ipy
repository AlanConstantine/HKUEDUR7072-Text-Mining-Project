# coding: utf-8
import pandas as pd
df = pd.read_csv('./reindex_df.csv')
df.shape
df[:7000].shape
df = df[:7000]
df.to_csv('./reindex7000.csv', index = False)
!ls
df.head()
import pandas as pd
import numpy as np
import time
from random import randint
import re
import string
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem.porter import *
stemmer = PorterStemmer()
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.utils import shuffle
import json
def pre_represent(ly):
    ly_tokenized_words = word_tokenize(ly)
    filtered_stopwords_ly = list(filter(lambda word: word not in stopwords.words('english'), ly_tokenized_words)) # removing stopwords
    punctutaion_ly = [stemmer.stem(w) for w in filtered_stopwords_ly if not re.fullmatch('[' + string.punctuation + ']+', w)]
    return ' '.join(punctutaion_ly)
    
df['represent'] = df['lyrics'].apply(pre_represent)
df['word_count'] = df['represent'].str.split().str.len()
df = df[df['word_count'] >= 100]
df = df[df['word_count'] <= 1000]
corpus = df['represent'].tolist()
vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = vectorizer.fit_transform(corpus)
df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names())
df_tfidf.head()
df.head()
corpus_text = ''
corpus_text = ' '.join(df['represent'].tolist())
from collections import Counter
word_count = Counter(word_list).most_common()
ly_tokenized_words = word_tokenize(corpus_text)
ly_tokenized_words
word_count = Counter(ly_tokenized_words).most_common()
word_count
word_count[-1]
len(word_count)
for i, v in word_count:
    print(i, v)
    break
    
filtered100 = list(filter(lambda k, v: v > 100, word_count))
filtered100 = list(filter(lambda v: v[1] > 100, word_count))
filtered100
filtered100[-1]
len(filtered100)
filtered100stpws = [v[0] for v in list(filter(lambda v: v[1] <= 100, word_count))]
filtered100stpws[1]
stopwords = nltk.corpus.stopwords.words('english')
stopwords.extend(filtered100stpws)
def secod_stpws(ly):
    ly_tokenized_words = word_tokenize(ly)
    filtered_stopwords_ly = list(filter(lambda word: word not in stopwords, ly_tokenized_words))
    return ' '.join(filtered_stopwords_ly)
    
df['represent_2nd'] = df['represent'].apply(secod_stpws)
corpus = df['represent_2nd'].tolist()
vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = vectorizer.fit_transform(corpus)
df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names())
df_tfidf.head()
df_tfidf['index'] = df_tfidf.index
df.shape
df.head()
%save -r mysession 1-99999
